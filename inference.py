# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Tl3cc8Mjj1Pq7zK8vQjqX9cC3g1YQ_n8

# **Importing Dependencies**
"""

import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    HfArgumentParser,
    TrainingArguments,
    pipeline,
    logging,
)

device_map = {"": 0}

"""# **Logging in to the huggingface hub**"""

from huggingface_hub import notebook_login
notebook_login()

# Reload model in FP16 and merge it with LoRA weights
model = AutoModelForCausalLM.from_pretrained(
    "unmolb/ChattoBotto_v2",
    low_cpu_mem_usage=True,
    return_dict=True,
    torch_dtype=torch.float16,
    device_map=device_map,
)
"""model = PeftModel.from_pretrained(base_model, new_model)
model = model.merge_and_unload()"""

# Reload tokenizer to save it
tokenizer = AutoTokenizer.from_pretrained("unmolb/ChattoBotto_v2", trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

"""# **Making Inference**"""

prompt = "bhavya didi kon hai?"
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=120)
result = pipe(f"[INST] {prompt} [/INST]")
print(result[0]['generated_text'])